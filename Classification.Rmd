---
title: "Classification"
author: "Benton Fariss"
date: "2023-02-18"
output: pdf_document
---

#Dataset from: https://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset
### How do linear models for classification work?
Logistic regression is used to model the relationship between a categorical dependent variable and a set of independent variables. We want to find linear boundaries between classes of data that allow us to predict the probability the probability of independent variables belonging to a specific class. Linear models for classification are best fit for large data sets and are very efficient and easily observable. 
#load data
```{r}
shoppers <- read.csv("C:/Users/setup/Downloads/online_shoppers_intention.csv")
```
### A. Split the Data into 80 train and 20 test
```{r}
set.seed(1234)
i <- sample(1:nrow(shoppers), nrow(shoppers)*.8, replace=FALSE)
train <- shoppers[i,]
test <- shoppers[-i,]
```
### B. Use 5 R functions for data exploration
This displays the structures of objects. 
```{r}
str(train)
```
This displays the first parts of the train dataset. 
```{r}
head(train)
```
This displays the last parts of the train dataset. 
```{r}
tail(train)
```
This displays the names of the objects in the train dataset. 
```{r}
names(train)
```
This displays the dimensions of the train dataset. 
```{r}
dim(train)
```
This displays the number of rows in the train dataset. 
```{r}
nrow(train)
```
This displays the number of columns in the train dataset. 
```{r}
ncol(train)
```
This displays the summaries of all of the values data frames. 
```{r}
summary(train)
```
### C. Create informative graphs using the training data
```{r}
ggplot(train, aes(x = factor(Revenue), y = ProductRelated_Duration)) +
  geom_bar(stat = "summary", fun = mean, fill = "black") +
  labs(title = "Mean Product Related Duration by Revenue",
       x = "Revenue", y = "Mean Product Related Duration (seconds)")
ggplot(train, aes(x = factor(Revenue), fill = Month)) +
  geom_bar(position = "fill") +
  labs(title = "Revenue by Month",
       x = "Revenue", y = "Proportion of Visits")
```
### D. Build a logistic regression model, output summary, and explain
```{r}
glm1 <- glm(Revenue~ProductRelated_Duration, data=train, family="binomial")
summary(glm1)
```
Deviance Residuals show the difference between the observed and predicted models. This means that when these numbers are lower, there is a low different in the predicted and actual probability. Coefficients show the change in the log odds of y for every 1 unit predictor change. The p-value is only acceptable if it is below .05. Null deviance is the measure of the response variable's entire variability. This is done using only the model's intercept. Residual deviance is the measure of the response variable's unexplained variability . Our Null and Residual deviance are not amazing, but they are okay. 
### E. Build a naïve Bayes model, output what the model learned, and explain
```{r}
library(e1071)
nb_model <- naive_bayes(Revenue~., data=train)
nb_model
plot(nb_model)
summary(nb_model)

```
This information tells us is the prior probabilities of Revenue were 84.47% False, and 15.53% True. This information allows us to much more easily make predictions on our new data. 
### F. Using these two classifications models models, predict and evaluate on the test data using all of the classification metrics from class. Compare and explain results. 
```{r}
probs <- predict(glm1, newdata=test, type="response")
pred <- ifelse(probs>.5,2,1)
acc1 <- mean(pred==as.integer(test$Revenue))
err1 <- 1-acc1
print(paste("glm1 accuracy= ", acc1))
print(paste("glm1 error: ", err1))
table(pred, as.integer(test$Revenue))

probs2 <- predict(nb_model, newdata=test, type="class")
acc2 <- mean(probs2==test$Revenue)
err2 <- 1-acc2
print(paste("nb_model accuracy:", acc2))
print(paste("nb_model error: ", err2))
table(probs2, test$Revenue)
```
The higher accuracy comes from the Naive Bayes model compared to the logistic regression. The accuracy of the logistic regression was .1496 and the accuracy of the Naive Bayes model was .8232. The logistic regression model did very poorly at about 15%. Our table shows us that this method should not even be consider, especially when comparing it to the vast difference in accuracy when using the NB model. 
### G. Write a paragraph explaining the strengths and weaknesses of the Naïve Bayes and Logistic Regression
The problem with Naive Bayes is that it assumes that all features are independent, which can limit the performance of the algorithm. It also does not work as well with larger data sets as something like logistic regression would. The last primary weakness is the guesses that Naive Bayes makes in the test set that do not happen in the training. The positives of it is that it is a great classifier for smaller data sets, it is simple, and it is great to use for multidimensional  tasks. The problem with logistic regression is that it is not work well with non-linear data, which leads to lower accuracy. The positives of logistic regression is that it works very well with linear data and is very cheap to run. 
### H. Write a paragraph listing the benefits, drawbacks of each of the classification metrics used
Accuracy measures the percentage of time that the classifier is predicting correctly. This is a great metric for the user to understand the data. Error rate shows us the percentage of time that the classifier is predicting incorrectly which can easily be calculated by the user, however it is important to see. This shows us the inaccuracies in the classifier and can help us decipher if it is significant. The confusion matrix shows a more complex performance analyzation using the true and false positives/negatives. This can also show us the accuracy and can help compute other metrcs such as sensitivity, specificity, and precision. 